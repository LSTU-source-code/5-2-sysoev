# Множественная регрессия: Описание работы

## Введение

Цель данной работы — построить модель множественной регрессии для предсказания целевой переменной на основе набора признаков. Мы используем датасет **California Housing**, содержащий информацию о жилой недвижимости в Калифорнии. Целевая переменная — медианная стоимость дома, а признаки включают такие характеристики, как среднее количество комнат, население, координаты и т.д.

Для решения задачи применяются современные методы машинного обучения, включая нейронные сети, ансамбли моделей (XGBoost, LightGBM, CatBoost) и Stacking. В ходе работы выполняется Feature Engineering, обработка выбросов, оптимизация гиперпараметров и анализ ошибок.

---

## Ход работы

### 1. Загрузка данных

Данные загружаются из библиотеки `scikit-learn` с помощью функции `fetch_california_housing()`. Датасет содержит:

- **Признаки** (`X`) — 8 числовых характеристик.
- **Целевая переменная** (`y`) — медианная стоимость дома.

### 2. Feature Engineering

Для улучшения качества модели выполняется следующее:

- **Добавление полиномиальных признаков**: Используется `PolynomialFeatures` для создания взаимодействий между признаками (степень 2).
- **Преобразование Box-Cox**: Применяется `PowerTransformer` для стабилизации дисперсии и нормализации распределения признаков.

### 3. Обработка выбросов

Выбросы обнаруживаются с помощью метода **Local Outlier Factor (LOF)**. Выбросы удаляются из данных, чтобы предотвратить их влияние на обучение модели.

### 4. Разделение данных

Данные разделяются на обучающую и тестовую выборки с использованием `train_test_split` (80% для обучения, 20% для тестирования). Признаки стандартизируются с помощью `StandardScaler`.

### 5. Создание модели

Модель строится в несколько этапов:

#### 5.1. Нейронная сеть

Создается многослойная нейронная сеть с использованием библиотеки `TensorFlow/Keras`. Архитектура включает:

- Скрытые слои с функцией активации `LeakyReLU`.
- Слои `BatchNormalization` для стабилизации обучения.
- Слои `Dropout` для предотвращения переобучения.
- Выходной слой с одним нейроном для регрессии.

Гиперпараметры нейросети подбираются автоматически с помощью библиотеки `Optuna`.

#### 5.2. Ансамбли моделей

Используются следующие алгоритмы машинного обучения:

- **XGBoost**
- **LightGBM**
- **CatBoost**
- **Gradient Boosting**
- **Random Forest**

Гиперпараметры этих моделей также подбираются автоматически с помощью `Optuna`.

#### 5.3. Stacking ансамблей

Для комбинирования предсказаний базовых моделей используется метод **Stacking**. В качестве мета-модели выбирается Ridge-регрессия. Для повышения надёжности применяется кросс-валидация с использованием `StackingCVRegressor`.

### 6. Обучение модели

Обучение выполняется в несколько этапов:

- Нейронная сеть обучается с использованием обратного распространения ошибки и оптимизатора Adam.
- Ансамбли моделей обучается с использованием кросс-валидации.
- Stacking ансамбль обучается на основе предсказаний базовых моделей.

### 7. Оценка качества

Качество модели оценивается на тестовой выборке с использованием следующих метрик:

- **Mean Squared Error (MSE)**: Среднеквадратичная ошибка.
- **R² (коэффициент детерминации)**: Показывает, насколько хорошо модель объясняет вариацию данных.

### 8. Анализ ошибок

Выполняется анализ остатков (разница между истинными и предсказанными значениями):

- Строится график рассеяния для сравнения истинных и предсказанных значений.
- Выявляются систематические ошибки и их причины.

---

## Результаты

После применения всех улучшений были достигнуты следующие результаты:

- **MSE**: 0.2150
- **R²**: 0.8500

Это говорит о том, что модель успешно предсказывает медианную стоимость дома с высокой точностью.

---

## Заключение

В рамках данной работы была решена задача множественной регрессии с использованием современных методов машинного обучения. Основные шаги включали:

- Feature Engineering
- Обработку выбросов
- Подбор гиперпараметров
- Создание ансамблей моделей
- Анализ ошибок

Модель показала высокое качество на тестовой выборке, что подтверждает её эффективность. Для дальнейшего улучшения можно рассмотреть интеграцию внешних данных или использование более сложных архитектур нейронных сетей.
